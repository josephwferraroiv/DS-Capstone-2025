# -*- coding: utf-8 -*-
"""Capstone_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LogFlR63P_zUehtt5fEUuo3QUeF-hJN3
"""

pip install yfinance pandas numpy scikit-learn matplotlib seaborn gym torch

import yfinance as yf
import pandas as pd

# Define tickers and date range
tickers = ['AAPL', 'RIOT', 'PLUG']
start_date = '2014-01-01'
end_date = '2024-12-31'

# Dictionary to store individual DataFrames
adj_close_data = {}

# Loop through each ticker
for ticker in tickers:
    data = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False)

    # Keep only 'Date' and 'Adj Close'
    df = data[['Adj Close']].reset_index()
    df.rename(columns={'Adj Close': f'{ticker}_AdjClose'}, inplace=True)

    adj_close_data[ticker] = df

# Example: show the first few rows of PLUG
# print(adj_close_data['PLUG'].head())

"""Clean Data next"""

for ticker, df in adj_close_data.items():
    # Drop rows with missing values
    df.dropna(inplace=True)

    # Convert 'Date' to datetime (in case it isn't already)
    df['Date'] = pd.to_datetime(df['Date'])

    # Sort chronologically
    df.sort_values(by='Date', inplace=True)

    # Reset index
    df.reset_index(drop=True, inplace=True)

    # Store back the cleaned DataFrame
    adj_close_data[ticker] = df

# Preview cleaned data
# print(adj_close_data['PLUG'].head())

"""compute technical indicators"""

# Loop through each symbol and compute indicators
for ticker, df in adj_close_data.items():
    # Use the adjusted close column
    price_col = f'{ticker}_AdjClose'

    # 20-day Simple Moving Average
    df['SMA_20'] = df[price_col].rolling(window=20).mean()

    # 10-day Momentum
    df['Momentum_10'] = df[price_col] - df[price_col].shift(10)

    # 10-day Volatility (standard deviation of past 10 days)
    df['Volatility_10'] = df[price_col].rolling(window=10).std()

    # Drop rows with NaN values resulting from rolling calculations
    df.dropna(inplace=True)

    # Reset index again after dropping rows
    df.reset_index(drop=True, inplace=True)

# Example: display indicators for TSLA
# print(adj_close_data['TSLA'][['Date', 'TSLA_AdjClose', 'SMA_20', 'Momentum_10', 'Volatility_10']].head())

"""Label Creation Code"""

for ticker, df in adj_close_data.items():
    price_col = f'{ticker}_AdjClose'

    # Compute forward 5-day return
    df['Forward_5d_Return'] = (df[price_col].shift(-5) - df[price_col]) / df[price_col]

    # Assign labels based on thresholds
    def label_return(r):
        if r > 0.02:
            return 1
        elif r < -0.02:
            return -1
        else:
            return 0

    df['Label'] = df['Forward_5d_Return'].apply(label_return)

    # Drop final rows with NaN from shift(-5)
    df.dropna(inplace=True)
    df.reset_index(drop=True, inplace=True)

# Example: show labeled data for AAPL
# print(adj_close_data['AAPL'][['Date', 'AAPL_AdjClose', 'Forward_5d_Return', 'Label']].head(10))

"""Training/Testing Set Split"""

# Dictionary to hold training and testing sets
train_test_split = {}

for ticker, df in adj_close_data.items():
    # Ensure 'Date' is datetime
    df['Date'] = pd.to_datetime(df['Date'])

    # Split the data
    train_df = df[df['Date'] < '2020-01-01'].copy()
    test_df = df[df['Date'] >= '2020-01-01'].copy()

    # Store in dictionary
    train_test_split[ticker] = {
        'train': train_df,
        'test': test_df
    }

# Example: view sizes of each split for TSLA
# print("TSLA Training set size:", len(train_test_split['TSLA']['train']))
# print("TSLA Testing set size:", len(train_test_split['TSLA']['test']))

"""Baseline Model #1: Random Guessing Model"""

import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

random_guess_results = {}
initial_capital = 100000

for ticker, splits in train_test_split.items():
    test_df = splits['test'].copy()

    # Generate random predictions
    test_df['Predicted_Action'] = np.random.choice([-1, 0, 1], size=len(test_df))

    # Simulate portfolio
    in_position = False
    entry_price = 0
    portfolio_value = float(initial_capital)
    portfolio_history = []

    for i, row in test_df.iterrows():
        action = int(row['Predicted_Action'])
        price = float(row[f'{ticker}_AdjClose'])

        if action == 1 and not in_position:
            entry_price = price
            in_position = True

        elif action == -1 and in_position:
            return_pct = (price - entry_price) / entry_price
            portfolio_value *= (1 + return_pct)
            in_position = False

        portfolio_history.append(portfolio_value)

    # If still in a position at the end, close it
    if in_position:
        final_price = float(test_df[f'{ticker}_AdjClose'].iloc[-1])
        return_pct = (final_price - entry_price) / entry_price
        portfolio_value *= (1 + return_pct)

    # Metrics
    accuracy = accuracy_score(test_df['Label'], test_df['Predicted_Action'])
    conf_matrix = confusion_matrix(test_df['Label'], test_df['Predicted_Action'], labels=[-1, 0, 1])
    class_report = classification_report(test_df['Label'], test_df['Predicted_Action'], labels=[-1, 0, 1], output_dict=True)

    random_guess_results[ticker] = {
        'final_portfolio_value': portfolio_value,
        'return_pct': (portfolio_value - initial_capital) / initial_capital,
        'accuracy': accuracy,
        'conf_matrix': conf_matrix,
        'classification_report': class_report
    }

    print(f"\n--- {ticker} [Random Guessing] ---")
    print(f"Final Portfolio Value: ${portfolio_value:,.2f}")
    print(f"Total Return: {(portfolio_value - initial_capital) / initial_capital:.2%}")
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:\n", conf_matrix)

"""Baseline Model #2: Buy and Hold Strategy"""

buy_hold_results = {}
initial_capital = 100000

for ticker, splits in train_test_split.items():
    test_df = splits['test'].copy()

    # Get adjusted close prices as floats
    entry_price = float(test_df[f'{ticker}_AdjClose'].iloc[0])
    exit_price = float(test_df[f'{ticker}_AdjClose'].iloc[-1])

    # Portfolio value after holding through test period
    return_pct = (exit_price - entry_price) / entry_price
    final_value = initial_capital * (1 + return_pct)

    # Simulate constant strategy:
    # Buy at start (1), hold (0) throughout, sell at end (-1)
    actions = [1] + [0] * (len(test_df) - 2) + [-1]
    test_df['Predicted_Action'] = actions

    # Evaluate prediction vs true labels
    accuracy = accuracy_score(test_df['Label'], test_df['Predicted_Action'])
    conf_matrix = confusion_matrix(test_df['Label'], test_df['Predicted_Action'], labels=[-1, 0, 1])
    class_report = classification_report(test_df['Label'], test_df['Predicted_Action'], labels=[-1, 0, 1], output_dict=True)

    buy_hold_results[ticker] = {
        'final_portfolio_value': final_value,
        'return_pct': return_pct,
        'accuracy': accuracy,
        'conf_matrix': conf_matrix,
        'classification_report': class_report
    }

    print(f"\n--- {ticker} [Buy and Hold] ---")
    print(f"Final Portfolio Value: ${final_value:,.2f}")
    print(f"Total Return: {return_pct:.2%}")
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:\n", conf_matrix)

"""Baseline Model #3 - Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Flatten column names for all tickers and both splits
for ticker, splits in train_test_split.items():
    for split_name in ['train', 'test']:
        df = splits[split_name]
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = ['_'.join(col).strip('_') for col in df.columns.values]
        splits[split_name] = df  # reassign flattened DataFrame

random_forest_results = {}
initial_capital = 100000
features = ['SMA_20', 'Momentum_10', 'Volatility_10']

for ticker, splits in train_test_split.items():
    train_df = splits['train'].copy()
    test_df = splits['test'].copy()

    # Drop rows with missing values in features or label
    train_df.dropna(subset=features + ['Label'], inplace=True)
    test_df.dropna(subset=features + ['Label'], inplace=True)

    # Prepare feature matrix and target vector
    X_train = train_df[features]
    y_train = train_df['Label']
    X_test = test_df[features]
    y_test = test_df['Label']

    # Train Random Forest
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)

    # Predict trading actions
    test_df['Predicted_Action'] = clf.predict(X_test)

    # üîç Find adjusted close column dynamically
    adj_close_col_candidates = [col for col in test_df.columns if 'AdjClose' in col and ticker in col]
    if not adj_close_col_candidates:
        raise KeyError(f"No adjusted close column found for {ticker}")
    adj_close_col = adj_close_col_candidates[0]

    # Simulate portfolio
    in_position = False
    entry_price = 0
    portfolio_value = float(initial_capital)
    portfolio_history = []

    for _, row in test_df.iterrows():
        action = int(row['Predicted_Action'])
        price = float(row[adj_close_col])

        if action == 1 and not in_position:
            entry_price = price
            in_position = True

        elif action == -1 and in_position:
            return_pct = (price - entry_price) / entry_price
            portfolio_value *= (1 + return_pct)
            in_position = False

        portfolio_history.append(portfolio_value)

    # Close any open position at end
    if in_position:
        final_price = float(test_df[adj_close_col].iloc[-1])
        return_pct = (final_price - entry_price) / entry_price
        portfolio_value *= (1 + return_pct)

    # Evaluation metrics
    accuracy = accuracy_score(test_df['Label'], test_df['Predicted_Action'])
    conf_matrix = confusion_matrix(test_df['Label'], test_df['Predicted_Action'], labels=[-1, 0, 1])
    class_report = classification_report(test_df['Label'], test_df['Predicted_Action'], labels=[-1, 0, 1], output_dict=True)

    # Store results
    random_forest_results[ticker] = {
        'final_portfolio_value': portfolio_value,
        'return_pct': (portfolio_value - initial_capital) / initial_capital,
        'accuracy': accuracy,
        'conf_matrix': conf_matrix,
        'classification_report': class_report
    }

    # Print summary
    print(f"\n--- {ticker} [Random Forest] ---")
    print(f"Final Portfolio Value: ${portfolio_value:,.2f}")
    print(f"Total Return: {(portfolio_value - initial_capital) / initial_capital:.2%}")
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:\n", conf_matrix)

"""**Reinforcement Learning Model**

Set up Gym-like Environment
"""

!pip install gym

import gym
from gym import spaces
import numpy as np

class TradingEnv(gym.Env):
    def __init__(self, df, initial_balance=100000):
        super(TradingEnv, self).__init__()
        self.df = df.reset_index(drop=True)
        self.initial_balance = initial_balance
        self.current_step = 0
        self.in_position = False
        self.entry_price = 0.0
        self.balance = initial_balance

        # Observation: [SMA, Momentum, Volatility, Position]
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)
        # Actions: 0=Hold, 1=Buy, 2=Sell
        self.action_space = spaces.Discrete(3)

    def _get_state(self):
        row = self.df.loc[self.current_step]
        return np.array([
            row['SMA_20'],
            row['Momentum_10'],
            row['Volatility_10'],
            float(self.in_position)
        ], dtype=np.float32)

    def step(self, action):
        done = False
        reward = 0
        row = self.df.loc[self.current_step]
        price = row[[col for col in self.df.columns if 'AdjClose' in col][0]]

        # Execute action
        if action == 1 and not self.in_position:  # Buy
            self.entry_price = price
            self.in_position = True
        elif action == 2 and self.in_position:  # Sell
            reward = (price - self.entry_price) / self.entry_price
            self.balance *= (1 + reward)
            self.in_position = False
        elif action != 0:
            reward = -0.001  # small penalty for invalid action (e.g., buying twice)

        self.current_step += 1
        if self.current_step >= len(self.df) - 1:
            done = True
            # Liquidate open position
            if self.in_position:
                price = self.df[[col for col in self.df.columns if 'AdjClose' in col][0]].iloc[self.current_step]
                reward = (price - self.entry_price) / self.entry_price
                self.balance *= (1 + reward)
                self.in_position = False

        return self._get_state(), reward, done, {}

    def reset(self):
        self.current_step = 0
        self.in_position = False
        self.entry_price = 0.0
        self.balance = self.initial_balance
        return self._get_state()

"""Set up DQN Agent using PyTorch"""

import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque

class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64), nn.ReLU(),
            nn.Linear(64, 64), nn.ReLU(),
            nn.Linear(64, output_dim)
        )

    def forward(self, x):
        return self.net(x)

class DQNAgent:
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.95, epsilon=1.0, eps_min=0.01, eps_decay=0.995):
        self.q_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        self.target_net.load_state_dict(self.q_net.state_dict())
        self.memory = deque(maxlen=5000)
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        self.gamma = gamma
        self.epsilon = epsilon
        self.eps_min = eps_min
        self.eps_decay = eps_decay
        self.action_dim = action_dim
        self.loss_fn = nn.MSELoss()

    def select_action(self, state):
        if random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        state = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            q_vals = self.q_net(state)
        return torch.argmax(q_vals).item()

    def store(self, transition):
        self.memory.append(transition)

    def update(self, batch_size=32):
        if len(self.memory) < batch_size:
            return

        batch = random.sample(self.memory, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions).unsqueeze(1)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)

        q_vals = self.q_net(states).gather(1, actions)
        with torch.no_grad():
            q_next = self.target_net(next_states).max(1)[0].unsqueeze(1)
            q_target = rewards + self.gamma * q_next * (1 - dones)

        loss = self.loss_fn(q_vals, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def update_target_network(self):
        self.target_net.load_state_dict(self.q_net.state_dict())

    def decay_epsilon(self):
        self.epsilon = max(self.eps_min, self.epsilon * self.eps_decay)

"""RL Testing Loop"""

def train_dqn(env, agent, episodes=50, batch_size=32, target_update_freq=10):
    for ep in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            action = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)
            agent.store((state, action, reward, next_state, done))
            agent.update(batch_size)
            state = next_state
            total_reward += reward

        agent.decay_epsilon()

        if ep % target_update_freq == 0:
            agent.update_target_network()

        print(f"Episode {ep+1}, Total Reward: {total_reward:.4f}, Epsilon: {agent.epsilon:.3f}")

from sklearn.metrics import accuracy_score, confusion_matrix

def evaluate_dqn(env, agent, ticker, initial_capital=100000):
    state = env.reset()
    done = False
    portfolio = [env.balance]
    predictions = []
    actuals = []

    while not done:
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        # Store predicted and true label
        predictions.append(action)
        actual_label = env.df.loc[env.current_step, 'Label'] if env.current_step < len(env.df) else 0
        actuals.append(actual_label)

        portfolio.append(env.balance)
        state = next_state

    final_value = env.balance
    return_pct = (final_value - initial_capital) / initial_capital
    accuracy = accuracy_score(actuals, predictions)
    conf_matrix = confusion_matrix(actuals, predictions, labels=[-1, 0, 1])

    print(f"\n--- {ticker} [DQN] ---")
    print(f"Final Portfolio Value: ${final_value:,.2f}")
    print(f"Total Return: {return_pct:.2%}")
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:\n", conf_matrix)

    return {
        'final_portfolio_value': final_value,
        'return_pct': return_pct,
        'accuracy': accuracy,
        'conf_matrix': conf_matrix,
        'portfolio_history': portfolio,
        'predictions': predictions,
        'actuals': actuals
    }

# Choose one symbol to train on
df = train_test_split['PLUG']['train'].copy()

# Environment and Agent setup
env = TradingEnv(df)
agent = DQNAgent(state_dim=4, action_dim=3)

# Train
train_dqn(env, agent)

# Evaluate on test data
test_env = TradingEnv(train_test_split['PLUG']['test'].copy())
evaluate_dqn(test_env, agent, ticker='PLUG')